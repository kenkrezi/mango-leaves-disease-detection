{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe56ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/lodhi/PycharmProjects/SSD_Keras/env_ssd/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from models.keras_ssd300 import ssd_300\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from keras_layers.keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layers.keras_layer_DecodeDetections import DecodeDetections\n",
    "from keras_layers.keras_layer_DecodeDetectionsFast import DecodeDetectionsFast\n",
    "from keras_layers.keras_layer_L2Normalization import L2Normalization\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from ssd_encoder_decoder.ssd_output_decoder import decode_detections, decode_detections_fast\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation\n",
    "from data_generator.object_detection_2d_misc_utils import apply_inverse_transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee82e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 300 # Height of the model input images\n",
    "img_width = 300 # Width of the model input images\n",
    "img_channels = 3 # Number of color channels of the model input images\n",
    "mean_color = [123, 117, 104] # The per-channel mean of the images in the dataset. Do not change this value if you're using any of the pre-trained weights.\n",
    "swap_channels = [2, 1, 0] # The color channel order in the original SSD is BGR, so we'll have the model reverse the color channel order of the input images.\n",
    "n_classes = 3 # Number of positive classes, e.g. 20 for Pascal VOC, 80 for MS COCO\n",
    "scales_pascal = [0.1, 0.2, 0.37, 0.54, 0.71, 0.88, 1.05] # The anchor box scaling factors used in the original SSD300 for the Pascal VOC datasets\n",
    "scales_coco = [0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05] # The anchor box scaling factors used in the original SSD300 for the MS COCO datasets\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5],\n",
    "                 [1.0, 2.0, 0.5]] # The anchor box aspect ratios used in the original SSD300; the order matters\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [8, 16, 32, 64, 100, 300] # The space between two adjacent anchor box center points for each predictor layer.\n",
    "offsets = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5] # The offsets of the first anchor box center points from the top and left borders of the image as a fraction of the step size for each predictor layer.\n",
    "clip_boxes = False # Whether or not to clip the anchor boxes to lie entirely within the image boundaries\n",
    "variances = [0.1, 0.1, 0.2, 0.2] # The variances by which the encoded target coordinates are divided as in the original implementation\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f15bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Build the Keras model.\n",
    "\n",
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_300(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)\n",
    "\n",
    "# 2: Load some weights into the model.\n",
    "\n",
    "# TODO: Set the path to the weights you want to load.\n",
    "weights_path = 'VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "# 3: Instantiate an optimizer and the SSD loss function and compile the model.\n",
    "#    If you want to follow the original Caffe implementation, use the preset SGD\n",
    "#    optimizer, otherwise I'd recommend the commented-out Adam optimizer.\n",
    "\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "sgd = SGD(lr=0.0005, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b51316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image set 'trainval.txt': 100%|██████████| 433/433 [00:02<00:00, 211.78it/s]\n",
      "Processing image set 'new_test.txt': 100%|██████████| 433/433 [00:00<00:00, 901.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1: Instantiate two `DataGenerator` objects: One for training, one for validation.\n",
    "\n",
    "# Optional: If you have enough memory, consider loading the images into memory for the reasons explained above.\n",
    "\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "# 2: Parse the image and label lists for the training and validation datasets. This can take a while.\n",
    "\n",
    "# TODO: Set the paths to the datasets here.\n",
    "\n",
    "# The directories that contain the images.\n",
    "VOC_2007_images_dir      = 'images' #done\n",
    "\n",
    "# The directories that contain the annotations.\n",
    "VOC_2007_annotations_dir      = 'Annotation' #done\n",
    "\n",
    "# The paths to the image sets.\n",
    "VOC_2007_train_image_set_filename    = 'new_train.txt' #done\n",
    "\n",
    "VOC_2007_val_image_set_filename      = 'val.txt'\n",
    "\n",
    "VOC_2007_trainval_image_set_filename = 'trainval.txt'\n",
    "\n",
    "VOC_2007_test_image_set_filename     = 'new_test.txt' #done\n",
    "\n",
    "# The XML parser needs to now what object class names to look for and in which order to map them to integers.\n",
    "classes = ['background','diseased','healthy']\n",
    "\n",
    "###########################################\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                        image_set_filenames=[VOC_2007_trainval_image_set_filename],\n",
    "                        annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "############################################\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2007_images_dir],\n",
    "                      image_set_filenames=[VOC_2007_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2007_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cd107a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset:\t   433\n",
      "Number of images in the validation dataset:\t   433\n"
     ]
    }
   ],
   "source": [
    "# 3: Set the batch size.\n",
    "\n",
    "batch_size = 16 # Change the batch size if you like, or if you run into GPU memory issues.\n",
    "\n",
    "# 4: Set the image transformations for pre-processing and data augmentation options.\n",
    "\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "\n",
    "# The encoder constructor needs the spatial dimensions of the model's predictor layers to create the anchor boxes.\n",
    "predictor_sizes = [model.get_layer('conv4_3_norm_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('fc7_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv6_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv7_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv8_2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('conv9_2_mbox_conf').output_shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=2,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=2,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size   = val_dataset.get_dataset_size()\n",
    "\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abba576-bd16-4876-8838-7073d5e2d8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d984b6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    if epoch < 80:\n",
    "        return 0.0005\n",
    "    elif epoch < 100:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab1bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='Mango.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "#model_checkpoint.best = \n",
    "\n",
    "csv_logger = CSVLogger(filename='Mango.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1bec515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 909s 2s/step - loss: 8.9323 - val_loss: 6.3945\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.39450, saving model to Mango.h5\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 885s 2s/step - loss: 7.0112 - val_loss: 5.5897\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.39450 to 5.58968, saving model to Mango.h5\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 856s 2s/step - loss: 6.0908 - val_loss: 5.3345\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.58968 to 5.33450, saving model to Mango.h5\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 813s 2s/step - loss: 5.6167 - val_loss: 4.6919\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.33450 to 4.69186, saving model to Mango.h5\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 865s 2s/step - loss: 5.4650 - val_loss: 4.2921\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.69186 to 4.29214, saving model to Mango.h5\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 839s 2s/step - loss: 5.2157 - val_loss: 4.1746\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.29214 to 4.17461, saving model to Mango.h5\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 5.1582 - val_loss: 4.2995\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.17461\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 823s 2s/step - loss: 5.0297 - val_loss: 3.9738\n",
      "\n",
      "Epoch 00008: val_loss improved from 4.17461 to 3.97384, saving model to Mango.h5\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 816s 2s/step - loss: 4.6495 - val_loss: 3.8295\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.97384 to 3.82946, saving model to Mango.h5\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 846s 2s/step - loss: 4.6294 - val_loss: 3.7103\n",
      "\n",
      "Epoch 00010: val_loss improved from 3.82946 to 3.71028, saving model to Mango.h5\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 847s 2s/step - loss: 4.6134 - val_loss: 3.8190\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.71028\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 874s 2s/step - loss: 4.5766 - val_loss: 3.9361\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.71028\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 831s 2s/step - loss: 4.5825 - val_loss: 3.6184\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.71028 to 3.61844, saving model to Mango.h5\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 829s 2s/step - loss: 4.3039 - val_loss: 3.4630\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.61844 to 3.46304, saving model to Mango.h5\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 831s 2s/step - loss: 4.3602 - val_loss: 3.5653\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.46304\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 853s 2s/step - loss: 4.2813 - val_loss: 3.5675\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.46304\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 841s 2s/step - loss: 4.1427 - val_loss: 3.3973\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.46304 to 3.39730, saving model to Mango.h5\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 874s 2s/step - loss: 4.2107 - val_loss: 3.4729\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.39730\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 793s 2s/step - loss: 4.1567 - val_loss: 3.6641\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.39730\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 860s 2s/step - loss: 4.0036 - val_loss: 4.0368\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 3.39730\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 828s 2s/step - loss: 4.1100 - val_loss: 3.2999\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.39730 to 3.29986, saving model to Mango.h5\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 829s 2s/step - loss: 4.1286 - val_loss: 3.2813\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.29986 to 3.28132, saving model to Mango.h5\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 859s 2s/step - loss: 3.9997 - val_loss: 3.3448\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3.28132\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 843s 2s/step - loss: 3.9708 - val_loss: 3.2940\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 3.28132\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 827s 2s/step - loss: 4.0599 - val_loss: 3.3742\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 3.28132\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 769s 2s/step - loss: 3.8082 - val_loss: 3.3562\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 3.28132\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 819s 2s/step - loss: 3.9551 - val_loss: 3.2512\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.28132 to 3.25117, saving model to Mango.h5\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 3.9323 - val_loss: 3.0341\n",
      "\n",
      "Epoch 00028: val_loss improved from 3.25117 to 3.03408, saving model to Mango.h5\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 830s 2s/step - loss: 3.7689 - val_loss: 3.0883\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.03408\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 843s 2s/step - loss: 3.8332 - val_loss: 3.3357\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 3.03408\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 856s 2s/step - loss: 3.7528 - val_loss: 3.2405\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.03408\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 812s 2s/step - loss: 3.7550 - val_loss: 3.2577\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.03408\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 880s 2s/step - loss: 3.8281 - val_loss: 3.2067\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 3.03408\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 835s 2s/step - loss: 3.7468 - val_loss: 3.1393\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 3.03408\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 848s 2s/step - loss: 3.6015 - val_loss: 3.0835\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.03408\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 817s 2s/step - loss: 3.8494 - val_loss: 3.0054\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.03408 to 3.00540, saving model to Mango.h5\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 822s 2s/step - loss: 3.6444 - val_loss: 3.2357\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3.00540\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 856s 2s/step - loss: 3.6787 - val_loss: 3.0712\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3.00540\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 795s 2s/step - loss: 3.5836 - val_loss: 3.0263\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.00540\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 825s 2s/step - loss: 3.5957 - val_loss: 3.0010\n",
      "\n",
      "Epoch 00040: val_loss improved from 3.00540 to 3.00104, saving model to Mango.h5\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 805s 2s/step - loss: 3.6188 - val_loss: 3.2880\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.00104\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 798s 2s/step - loss: 3.5457 - val_loss: 2.8609\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.00104 to 2.86086, saving model to Mango.h5\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 862s 2s/step - loss: 3.5458 - val_loss: 2.8976\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.86086\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 786s 2s/step - loss: 3.5598 - val_loss: 3.0161\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.86086\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 823s 2s/step - loss: 3.4806 - val_loss: 2.9005\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.86086\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 841s 2s/step - loss: 3.5351 - val_loss: 2.9550\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.86086\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 776s 2s/step - loss: 3.3903 - val_loss: 3.1352\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.86086\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 834s 2s/step - loss: 3.4752 - val_loss: 2.9829\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.86086\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 808s 2s/step - loss: 3.4095 - val_loss: 2.8719\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.86086\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 819s 2s/step - loss: 3.3725 - val_loss: 2.9935\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.86086\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 784s 2s/step - loss: 3.4256 - val_loss: 2.8774\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.86086\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 841s 2s/step - loss: 3.4527 - val_loss: 2.9705\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.86086\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 794s 2s/step - loss: 3.4572 - val_loss: 3.1757\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.86086\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 846s 2s/step - loss: 3.4625 - val_loss: 2.8958\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.86086\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 3.3182 - val_loss: 2.7898\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.86086 to 2.78984, saving model to Mango.h5\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 842s 2s/step - loss: 3.3493 - val_loss: 2.7803\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.78984 to 2.78035, saving model to Mango.h5\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 831s 2s/step - loss: 3.3201 - val_loss: 2.6786\n",
      "\n",
      "Epoch 00057: val_loss improved from 2.78035 to 2.67855, saving model to Mango.h5\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 804s 2s/step - loss: 3.2969 - val_loss: 2.8014\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.67855\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 816s 2s/step - loss: 3.5701 - val_loss: 3.1506\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.67855\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 815s 2s/step - loss: 3.2309 - val_loss: 2.7173\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.67855\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 800s 2s/step - loss: 3.2512 - val_loss: 2.7279\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.67855\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 801s 2s/step - loss: 3.2364 - val_loss: 2.6711\n",
      "\n",
      "Epoch 00062: val_loss improved from 2.67855 to 2.67111, saving model to Mango.h5\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 846s 2s/step - loss: 3.2751 - val_loss: 2.7668\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.67111\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 851s 2s/step - loss: 3.2916 - val_loss: 2.6816\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.67111\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 791s 2s/step - loss: 3.2009 - val_loss: 2.6848\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.67111\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 854s 2s/step - loss: 3.2502 - val_loss: 2.7070\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.67111\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 829s 2s/step - loss: 3.2424 - val_loss: 2.6888\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.67111\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 836s 2s/step - loss: 3.1915 - val_loss: 2.5821\n",
      "\n",
      "Epoch 00068: val_loss improved from 2.67111 to 2.58206, saving model to Mango.h5\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 833s 2s/step - loss: 3.1593 - val_loss: 2.7157\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.58206\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 773s 2s/step - loss: 3.1812 - val_loss: 2.6861\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.58206\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 811s 2s/step - loss: 3.1199 - val_loss: 2.6687\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.58206\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 860s 2s/step - loss: 3.1607 - val_loss: 2.6332\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.58206\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 798s 2s/step - loss: 3.2052 - val_loss: 2.6036\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.58206\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 844s 2s/step - loss: 3.1866 - val_loss: 2.5694\n",
      "\n",
      "Epoch 00074: val_loss improved from 2.58206 to 2.56937, saving model to Mango.h5\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 3.1861 - val_loss: 2.5967\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.56937\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 786s 2s/step - loss: 3.0122 - val_loss: 2.7863\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.56937\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 833s 2s/step - loss: 3.0909 - val_loss: 2.6336\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.56937\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 813s 2s/step - loss: 3.1501 - val_loss: 2.6807\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.56937\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 834s 2s/step - loss: 3.0763 - val_loss: 2.4536\n",
      "\n",
      "Epoch 00079: val_loss improved from 2.56937 to 2.45364, saving model to Mango.h5\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "500/500 [==============================] - 813s 2s/step - loss: 3.0088 - val_loss: 2.3956\n",
      "\n",
      "Epoch 00080: val_loss improved from 2.45364 to 2.39560, saving model to Mango.h5\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 811s 2s/step - loss: 2.8822 - val_loss: 2.3791\n",
      "\n",
      "Epoch 00081: val_loss improved from 2.39560 to 2.37914, saving model to Mango.h5\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 836s 2s/step - loss: 2.8914 - val_loss: 2.3930\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.37914\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 789s 2s/step - loss: 2.8426 - val_loss: 2.3923\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.37914\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 2.8255 - val_loss: 2.4469\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.37914\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 833s 2s/step - loss: 2.8359 - val_loss: 2.3375\n",
      "\n",
      "Epoch 00085: val_loss improved from 2.37914 to 2.33745, saving model to Mango.h5\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 854s 2s/step - loss: 2.7910 - val_loss: 2.4058\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.33745\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 858s 2s/step - loss: 2.7439 - val_loss: 2.3128\n",
      "\n",
      "Epoch 00087: val_loss improved from 2.33745 to 2.31281, saving model to Mango.h5\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 809s 2s/step - loss: 2.7975 - val_loss: 2.3006\n",
      "\n",
      "Epoch 00088: val_loss improved from 2.31281 to 2.30060, saving model to Mango.h5\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 834s 2s/step - loss: 2.7971 - val_loss: 2.3117\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2.30060\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 2.7290 - val_loss: 2.3344\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.30060\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 843s 2s/step - loss: 2.7740 - val_loss: 2.3682\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.30060\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 822s 2s/step - loss: 2.7954 - val_loss: 2.3285\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.30060\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 814s 2s/step - loss: 2.7112 - val_loss: 2.4148\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.30060\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 863s 2s/step - loss: 2.7446 - val_loss: 2.2558\n",
      "\n",
      "Epoch 00094: val_loss improved from 2.30060 to 2.25582, saving model to Mango.h5\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 832s 2s/step - loss: 2.6971 - val_loss: 2.2892\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.25582\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 810s 2s/step - loss: 2.6979 - val_loss: 2.2829\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.25582\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 820s 2s/step - loss: 2.7586 - val_loss: 2.3013\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.25582\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 811s 2s/step - loss: 2.7460 - val_loss: 2.3483\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.25582\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 815s 2s/step - loss: 2.7383 - val_loss: 2.3522\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.25582\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "500/500 [==============================] - 826s 2s/step - loss: 2.7267 - val_loss: 2.2939\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.25582\n"
     ]
    }
   ],
   "source": [
    "initial_epoch   = 0\n",
    "final_epoch     = 100\n",
    "steps_per_epoch = 500\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('new_SSD.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c927a9b-b872-4ee7-bdf9-d51c8229e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Mango_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68f0d-6353-4627-9025-1df2a898f21d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
